{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from torchviz import make_dot\n",
    "from itertools import islice\n",
    "import torch\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import hiddenlayer as hl\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "g = graphviz.Graph(engine='neato')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(str):\n",
    "    '''\n",
    "    :param str: string representing a list of centipawn losses\n",
    "    :return: list of integer centipawn losses\n",
    "    '''\n",
    "    string = str.replace('[','').replace(']','')\n",
    "    ls = string.split(',')\n",
    "    list = [int(i) for i in ls]\n",
    "\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def black_process(eval):\n",
    "    '''\n",
    "    :param eval: list of integer centipawn losses\n",
    "    :return: array of lists of [evaluation, centipawn loss]\n",
    "    '''\n",
    "\n",
    "    # starting evaluation of 30 centipawns\n",
    "    sum = 30\n",
    "\n",
    "    i = 0\n",
    "    res = []\n",
    "\n",
    "    # iterating through centipawn losses\n",
    "    for cpl in eval:\n",
    "\n",
    "        # subtracting the cpl for white's moves\n",
    "        if i % 2 ==0:\n",
    "            sum -= cpl\n",
    "            i += 1\n",
    "\n",
    "        # adding the cpl for black's moves\n",
    "        else:\n",
    "            res.append([sum,cpl])\n",
    "            sum+=cpl\n",
    "            i+=1\n",
    "\n",
    "    return numpy.array(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_process(eval):\n",
    "    '''\n",
    "    :param eval: list of integer centipawn losses\n",
    "    :return: array of lists of [evaluation, centipawn loss]\n",
    "    '''\n",
    "\n",
    "    # starting evaluation of 30 centipawns\n",
    "    sum = 30\n",
    "\n",
    "    i = 0\n",
    "    res = []\n",
    "\n",
    "    # iterating through centipawn losses\n",
    "    for cpl in eval:\n",
    "\n",
    "        # subtracting the cpl for white's moves\n",
    "        if i % 2 ==0:\n",
    "            res.append([sum,cpl])\n",
    "            sum -= cpl\n",
    "            i += 1\n",
    "\n",
    "        # adding the cpl for black's moves\n",
    "        else:\n",
    "            sum+=cpl\n",
    "            i+=1\n",
    "\n",
    "    return numpy.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  games: 4469\n",
      "Evaluted games: 458\n"
     ]
    }
   ],
   "source": [
    "# reading *some* of the data\n",
    "dfs = []\n",
    "\n",
    "players = ['andreikin, dmitry', 'anand, viswanathan', 'wang, hao', 'grischuk, alexander', 'karjakin, sergey','duda, jan-krzysztof', 'radjabov, teimour', 'dominguez perez, leinier','nakamura, hikaru', 'vachier-lagrave, maxime','aronian, levon','mamedyarov, shakhriyar', 'so, wesley','ding, liren', 'rapport, richard', 'nepomniachtchi, ian', 'giri, anish', 'firouzja, alireza', 'caruana, fabiano','carlsen, magnus','zelcic, robert','khotenashvili, bela', 'bischoff, klaus', 'hoffmann, asa','kaufman, lawrence','bellaiche, elise']\n",
    "\n",
    "# reading the csvs\n",
    "for player in players:\n",
    "    df = pd.read_csv('blitz/'+player +'.csv')\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "print(f\"Total  games: {len(df)}\")\n",
    "\n",
    "# Filtering out * values\n",
    "df = df[df['WhiteELO'] != '*']\n",
    "df = df[df['BlackELO'] != '*']\n",
    "df[['WhiteELO', 'BlackELO']] = df[['WhiteELO', 'BlackELO']] .astype(int)\n",
    "\n",
    "df = df[df['Eval'] != '']\n",
    "df = df[ df['Eval'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "\n",
    "# converting the evaluation to a list\n",
    "df['Eval'] = df['Eval'].apply( to_list)\n",
    "df['WhiteEval'] = df['Eval'].apply( white_process )\n",
    "df['BlackEval'] = df['Eval'].apply( black_process )\n",
    "\n",
    "print(f\"Evaluted games: {len(df['Eval'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_x = numpy.array(df['WhiteEval'])\n",
    "white_length = numpy.array(df['WhiteEval'].apply(len))\n",
    "\n",
    "black_x = numpy.array(df['WhiteEval'])\n",
    "black_length = numpy.array(df['WhiteEval'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and fitting a power transformer for black and white\n",
    "wpt = PowerTransformer()\n",
    "white_y = numpy.concatenate(white_x)\n",
    "wpt.fit(white_y)\n",
    "white_transformed = wpt.transform(white_y)\n",
    "\n",
    "bpt = PowerTransformer()\n",
    "black_y = numpy.concatenate(black_x)\n",
    "bpt.fit(black_y)\n",
    "black_transformed = bpt.transform(black_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a function now to (effieciently) change these to lists of length list\n",
    "white_transformed_array = [numpy.array(list(islice(iter(white_transformed), elem)))\n",
    "        for elem in white_length]\n",
    "\n",
    "black_transformed_array = [numpy.array(list(islice(iter(black_transformed), elem)))\n",
    "        for elem in black_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique evaluated games: 446\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique evaluated games: {df['Game'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_472\\4203895052.py:2: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  white_evals = [torch.tensor(i, dtype = torch.float32) for i in white_transformed_array]\n"
     ]
    }
   ],
   "source": [
    "# converting evaluations and length to tensors\n",
    "white_evals = [torch.tensor(i, dtype = torch.float32) for i in white_transformed_array]\n",
    "white_lengths = [len(tensor) for tensor in white_evals]\n",
    "\n",
    "black_evals = [torch.tensor(i, dtype = torch.float32) for i in black_transformed_array]\n",
    "black_lengths = [len(tensor) for tensor in black_evals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding my sequences - not sure why batch first works, but it does\n",
    "#inputs = torch.nn.utils.rnn.pad_sequence(evals, batch_first=True, padding_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs_array = numpy.array(inputs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs_list =inputs.tolist()\n",
    "\n",
    "# normalizing... a bit hacky\n",
    "#inputs_array = (numpy.array(inputs_list) - numpy.array(inputs_list).mean())/ numpy.linalg.norm(numpy.array(inputs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    '''\n",
    "    :param array:\n",
    "    :return:\n",
    "    '''\n",
    "    return (array - array.mean())/array.std()\n",
    "\n",
    "def denormalize(array, value):\n",
    "    '''\n",
    "    :param array:\n",
    "    :param value:\n",
    "    :return:\n",
    "    '''\n",
    "    return value*array.std() + array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['WhiteELO'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting White and Black's ELOs to tensors\n",
    "white_elo_arr = numpy.array(df['WhiteELO'])\n",
    "\n",
    "white_elo = normalize(white_elo_arr)\n",
    "\n",
    "#print(white_elo)\n",
    "white_elo = [torch.tensor(i, dtype = torch.float32) for i in white_elo]\n",
    "\n",
    "\n",
    "\n",
    "black_elo = numpy.array(df['BlackELO'])\n",
    "black_elo = [torch.tensor(i, dtype = torch.float32) for i in black_elo]\n",
    "\n",
    "\n",
    "# splitting into train and test\n",
    "#lengths_train, lengths_test,white_eval_train, white_eval_test, black_eval_train, black_eval_test, black_train, black_test, white_train, white_test  = train_test_split(lengths, white_evals, black_evals, black_elo, white_elo, test_size=0.2,random_state=0, shuffle = True)\n",
    "white_eval_train, white_eval_test, black_eval_train, black_eval_test, black_train, black_test, white_train, white_test  = train_test_split(white_evals, black_evals, black_elo, white_elo, test_size=0.2,random_state=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping the elo together with the evaluations\n",
    "train_data_zip = list(zip(white_eval_train, black_eval_train, white_train))\n",
    "test_data_zip = list(zip(white_eval_test, black_eval_test, white_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_elo = torch.stack(black_elo)\n",
    "white_elo = torch.stack(white_elo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, no_layers):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.no_layers = no_layers\n",
    "        self.white_rnn = nn.RNN(input_size, hidden_size, no_layers, batch_first = True, bias = True)\n",
    "        self.black_rnn = nn.RNN(input_size, hidden_size, no_layers, batch_first = True, bias = True)\n",
    "\n",
    "        self.white_fc = nn.Linear(hidden_size,1, bias = False)\n",
    "        self.black_fc = nn.Linear(hidden_size,1, bias = False)\n",
    "\n",
    "        self.final_fc = nn.Linear(2,1, bias = False)\n",
    "\n",
    "        self.final = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        white, black = x\n",
    "        white_out, _ = self.white_rnn(white)\n",
    "        white_output ,white_lengths= torch.nn.utils.rnn.pad_packed_sequence(white_out, batch_first = True)\n",
    "\n",
    "        # shape batches, seq_length, hidden_size\n",
    "        white_out = [white_output[e, i-1,:].unsqueeze(0)for e, i in enumerate(white_lengths)]\n",
    "        white_out = torch.cat(white_out, dim = 0)\n",
    "\n",
    "        white_out = self.white_fc(white_out)\n",
    "\n",
    "        # Doing the same for black\n",
    "        black_out, _ = self.black_rnn(black)\n",
    "        black_output, black_lengths= torch.nn.utils.rnn.pad_packed_sequence(black_out, batch_first = True)\n",
    "\n",
    "        # shape batches, seq_length, hidden_size\n",
    "        black_out = [black_output[e, i-1,:].unsqueeze(0)for e, i in enumerate(black_lengths)]\n",
    "        black_out = torch.cat(black_out, dim = 0)\n",
    "\n",
    "        black_out = self.black_fc(black_out)\n",
    "\n",
    "        #black_out = black_out.squeeze()\n",
    "        #white_out = white_out.squeeze()\n",
    "\n",
    "\n",
    "        out = torch.stack([white_out, black_out], dim=1)\n",
    "        print(out.shape)\n",
    "        out = out.squeeze()\n",
    "        out = out.unsqueeze(1)\n",
    "        print(out.shape)\n",
    "        out = self.final_fc(out)\n",
    "\n",
    "        out = self.final(out)\n",
    "\n",
    "        #print(out.shape)\n",
    "        out = out[:,0]\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "    #def init_hidden(self):\n",
    "    #    return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n",
    "# need to figure out exactly how the dimensions changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollator(object):\n",
    "    '''\n",
    "    Yields a batch from a list of Items\n",
    "    Args:\n",
    "    test : Set True when using with test data loader. Defaults to False\n",
    "    percentile : Trim sequences by this percentile\n",
    "    '''\n",
    "\n",
    "    # remove that eventually. I'm going to need to make my dataset a tuple with evals and elo\n",
    "    #def __init__(self):\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        white_data = [item[0] for item in batch]\n",
    "        black_data = [item[1] for item in batch]\n",
    "        target = [item[2] for item in batch]\n",
    "        white_lens = [i.shape[0] for i in white_data]\n",
    "        black_lens = [i.shape[0] for i in black_data]\n",
    "\n",
    "\n",
    "        white_data = torch.nn.utils.rnn.pad_sequence(white_data, batch_first=True,padding_value = 0)\n",
    "        white_evals_packed = torch.nn.utils.rnn.pack_padded_sequence(white_data,batch_first = True, lengths=white_lens,enforce_sorted=False)\n",
    "\n",
    "        black_data = torch.nn.utils.rnn.pad_sequence(black_data, batch_first=True,padding_value = 0)\n",
    "        black_evals_packed = torch.nn.utils.rnn.pack_padded_sequence(black_data,batch_first = True, lengths=black_lens,enforce_sorted=False)\n",
    "\n",
    "\n",
    "        target = torch.tensor(target,dtype=torch.float32)\n",
    "        return [white_evals_packed, black_evals_packed,target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 45\n",
    "no_layers = 4\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (defining my model)\n",
    "model = MyRNN(input_size, hidden_size, no_layers)\n",
    "collate = MyCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of MyRNN(\n",
      "  (white_rnn): RNN(2, 45, num_layers=4, batch_first=True)\n",
      "  (black_rnn): RNN(2, 45, num_layers=4, batch_first=True)\n",
      "  (white_fc): Linear(in_features=45, out_features=1, bias=False)\n",
      "  (black_fc): Linear(in_features=45, out_features=1, bias=False)\n",
      "  (final_fc): Linear(in_features=2, out_features=1, bias=False)\n",
      "  (final): Tanh()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/h45l4-2')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "# # add to loop\n",
    "# running_loss  = 0\n",
    "# running_loss += loss.item()\n",
    "# writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps +i)\n",
    "# writer.add_scalar('accuracy', running_loss / 100, epoch * n_total_steps +i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train_data_zip, batch_size=batch_size, shuffle=True ,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK I've figured out the issue, I also need the sequence length for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 1, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([1, 2, 1])\n",
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1 and 2x1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [338]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m evals \u001B[38;5;241m=\u001B[39m (white_evals, black_evals)\n\u001B[0;32m     19\u001B[0m elo \u001B[38;5;241m=\u001B[39m elo\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 21\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m#print(outputs)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#print(outputs.shape, elo.shape)\u001B[39;00m\n\u001B[0;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs,elo)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [326]\u001B[0m, in \u001B[0;36mMyRNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     44\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(out\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 46\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinal_fc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal(out)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m#print(out.shape)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x1)"
     ]
    }
   ],
   "source": [
    "avg_losses = []\n",
    "epochs = []\n",
    "avg_loss = 1\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    if (epoch+1) % 10 ==0:\n",
    "        learning_rate /= 2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    i = 0\n",
    "    for white_evals, black_evals, elo in data_loader:\n",
    "\n",
    "        white_evals = white_evals.to(device)\n",
    "        black_evals = black_evals.to(device)\n",
    "        evals = (white_evals, black_evals)\n",
    "        elo = elo.to(device)\n",
    "\n",
    "        outputs = model(evals)\n",
    "        #print(outputs)\n",
    "        #print(outputs.shape, elo.shape)\n",
    "        loss = criterion(outputs,elo)\n",
    "        # optimizing\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        i+=1\n",
    "\n",
    "    change = stats.mean(losses)/avg_loss\n",
    "    avg_loss = stats.mean(losses)\n",
    "\n",
    "    # adding histograms to the summary writer\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(name, np.array(param.detach().tolist()), epoch)\n",
    "\n",
    "    # adding loss\n",
    "    writer.add_scalar('Average loss',avg_loss, epoch)\n",
    "    avg_losses.append(avg_loss)\n",
    "    epochs.append(epoch)\n",
    "    print(f'Epoch {epoch+1} step {i+1} - Learning Rate : {learning_rate}- Avg Loss: {avg_loss:3f} - Change in loss: {change}')\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(epochs, avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model =model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = torch.utils.data.DataLoader(test_data_zip, batch_size=1, shuffle=False ,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "outputs = []\n",
    "elos = []\n",
    "for evals, elo in test_data_loader:\n",
    "    #print(\"evals\",evals.shape)\n",
    "    evals = evals.to(device)\n",
    "    elo = elo.to(device)\n",
    "    output = model(evals)\n",
    "    outputs.append(output.item())\n",
    "    elos.append(elo.item())\n",
    "    loss = criterion(output,elo)\n",
    "    #print(f'Model prediction : {output} \\n ELO : {elo} \\n MSE : {loss}')\n",
    "    losses.append(loss.item())\n",
    "print(f'Average loss : {stats.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(elos,outputs, alpha = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the neural net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(outputs))\n",
    "print(min(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "#traced_script_module = torch.jit.trace(model, example)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params =dict(model.named_parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dot = make_dot(output, params=params, show_attrs=False, show_saved=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dot.render(\"rnn_torchviz3\", format=\"pdf\", engine= 'neato') # doesn't seem to work great with padded & packed input..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/run1')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
