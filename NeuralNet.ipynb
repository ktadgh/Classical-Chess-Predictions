{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from torchviz import make_dot\n",
    "from itertools import islice\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "def to_list(str):\n",
    "    '''\n",
    "    :param str: string representing a list of centipawn losses\n",
    "    :return: list of integer centipawn losses\n",
    "    '''\n",
    "    string = str.replace('[','').replace(']','')\n",
    "    ls = string.split(',')\n",
    "    list = [int(i) for i in ls]\n",
    "\n",
    "    return list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "def process(eval):\n",
    "    '''\n",
    "    :param eval: list of integer centipawn losses\n",
    "    :return: array of lists of [evaluation, centipawn loss]\n",
    "    '''\n",
    "\n",
    "    # starting evaluation of 30 centipawns\n",
    "    sum = 30\n",
    "\n",
    "    i = 0\n",
    "    res = []\n",
    "\n",
    "    # iterating through centipawn losses\n",
    "    for cpl in eval:\n",
    "\n",
    "        # subtracting the cpl for white's moves\n",
    "        if i % 2 ==0:\n",
    "            res.append([sum,cpl])\n",
    "            sum -= cpl\n",
    "            i += 1\n",
    "\n",
    "        # adding the cpl for black's moves\n",
    "        else:\n",
    "            sum+=cpl\n",
    "            i+=1\n",
    "\n",
    "    return np.array(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  games: 4210\n",
      "Evaluted games: 434\n"
     ]
    }
   ],
   "source": [
    "# reading *some* of the data\n",
    "dfs = []\n",
    "\n",
    "players = ['andreikin, dmitry', 'anand, viswanathan', 'wang, hao', 'grischuk, alexander', 'karjakin, sergey','duda, jan-krzysztof', 'radjabov, teimour', 'dominguez perez, leinier','nakamura, hikaru', 'vachier-lagrave, maxime','aronian, levon','mamedyarov, shakhriyar', 'so, wesley','ding, liren', 'rapport, richard', 'nepomniachtchi, ian', 'giri, anish', 'firouzja, alireza', 'caruana, fabiano','carlsen, magnus','zelcic, robert','khotenashvili, bela', 'bischoff, klaus', 'hoffmann, asa','kaufman, lawrence','bellaiche, elise']\n",
    "\n",
    "# reading the csvs\n",
    "for player in players:\n",
    "    df = pd.read_csv('blitz/'+player +'.csv')\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "print(f\"Total  games: {len(df)}\")\n",
    "\n",
    "# Filtering out * values\n",
    "df = df[df['WhiteELO'] != '*']\n",
    "df = df[df['BlackELO'] != '*']\n",
    "df[['WhiteELO', 'BlackELO']] = df[['WhiteELO', 'BlackELO']] .astype(int)\n",
    "\n",
    "df = df[df['Eval'] != '']\n",
    "df = df[ df['Eval'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "\n",
    "# converting the evaluation to a list\n",
    "df['Eval'] = df['Eval'].apply( to_list)\n",
    "df['Eval'] = df['Eval'].apply( process )\n",
    "\n",
    "print(f\"Evaluted games: {len(df['Eval'])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "x = np.array(df['Eval'])\n",
    "length = np.array(df['Eval'].apply(len))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "# creating and fitting a power transformer\n",
    "pt = PowerTransformer()\n",
    "y = np.concatenate(x)\n",
    "pt.fit(y)\n",
    "transformed = pt.transform(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "# need a function now to (effieciently) change these to lists of length list\n",
    "transformed_array = [np.array(list(islice(iter(transformed), elem)))\n",
    "        for elem in length]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique evaluated games: 422\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique evaluated games: {df['Game'].nunique()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Preparing the data for the Neural Net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "# converting evaluations and length to tensors\n",
    "evals = [torch.tensor(i, dtype = torch.float32) for i in transformed_array]\n",
    "lengths = [len(tensor) for tensor in evals]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "# Padding my sequences - not sure why batch first works, but it does\n",
    "#inputs = torch.nn.utils.rnn.pad_sequence(evals, batch_first=True, padding_value=0.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "#inputs_array = np.array(inputs.tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "#print(inputs.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "#inputs_list =inputs.tolist()\n",
    "\n",
    "# normalizing... a bit hacky\n",
    "#inputs_array = (np.array(inputs_list) - np.array(inputs_list).mean())/ np.linalg.norm(np.array(inputs_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    '''\n",
    "    :param array:\n",
    "    :return:\n",
    "    '''\n",
    "    return (array - array.mean())/array.std()\n",
    "\n",
    "def denormalize(array, value):\n",
    "    '''\n",
    "    :param array:\n",
    "    :param value:\n",
    "    :return:\n",
    "    '''\n",
    "    return value*array.std() + array.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "#print(df['WhiteELO'].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "# Converting White and Black's ELOs to tensors\n",
    "white_elo_arr = np.array(df['WhiteELO'])\n",
    "\n",
    "white_elo = normalize(white_elo_arr)\n",
    "\n",
    "#print(white_elo)\n",
    "white_elo = [torch.tensor(i, dtype = torch.float32) for i in white_elo]\n",
    "\n",
    "\n",
    "\n",
    "black_elo = np.array(df['BlackELO'])\n",
    "black_elo = [torch.tensor(i, dtype = torch.float32) for i in black_elo]\n",
    "\n",
    "\n",
    "# splitting into train and test\n",
    "lengths_train, lengths_test,eval_train, eval_test, black_train, black_test, white_train, white_test  = train_test_split(lengths, evals, black_elo, white_elo, test_size=0.2,random_state=0, shuffle = True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "# zipping the elo together with the evaluations\n",
    "train_data_zip = list(zip(eval_train, white_train))\n",
    "test_data_zip = list(zip(eval_test, white_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "black_elo = torch.stack(black_elo)\n",
    "white_elo = torch.stack(white_elo)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the Neural Net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, no_layers, batch_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.no_layers = no_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, no_layers, batch_first = True, bias = True)\n",
    "        self.fc = nn.Linear(hidden_size,1, bias = False)\n",
    "        self.final = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        t, l = torch.nn.utils.rnn.pad_packed_sequence(x)\n",
    "\n",
    "        # initial hidden state\n",
    "        #h0 = torch.rand(self.no_layers,batch_size,self.hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "\n",
    "        # shape batches, seq_length, hidden_size\n",
    "        output ,lengths = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first = True)\n",
    "\n",
    "        out = [output[e, i-1,:].unsqueeze(0) for e, i in enumerate(lengths)]\n",
    "\n",
    "        out = torch.cat(out, dim = 0)\n",
    "        #print(out.shape)\n",
    "        #print(\"Linear weights\", self.fc.weight)\n",
    "\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = self.final(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out[:,0]\n",
    "\n",
    "\n",
    "    #def init_hidden(self):\n",
    "    #    return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n",
    "# need to figure out exactly how the dimensions changed\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "class MyCollator(object):\n",
    "    '''\n",
    "    Yields a batch from a list of Items\n",
    "    Args:\n",
    "    test : Set True when using with test data loader. Defaults to False\n",
    "    percentile : Trim sequences by this percentile\n",
    "    '''\n",
    "\n",
    "    # remove that eventually. I'm going to need to make my dataset a tuple with evals and elo\n",
    "    #def __init__(self):\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        data = [item[0] for item in batch]\n",
    "        target = [item[1] for item in batch]\n",
    "        lens = [i.shape[0] for i in data]\n",
    "\n",
    "        #print(lens)\n",
    "        data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True,padding_value = 0)\n",
    "        #data = data.unsqueeze(2)\n",
    "        #print(\"PADDED\",data)\n",
    "        evals_packed = torch.nn.utils.rnn.pack_padded_sequence(data,batch_first = True, lengths=lens,enforce_sorted=False)\n",
    "\n",
    "        target = torch.tensor(target,dtype=torch.float32)\n",
    "        return [evals_packed,target]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 256\n",
    "no_layers = 3\n",
    "batch_size =5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "# (defining my model)\n",
    "model = MyRNN(input_size, hidden_size, no_layers, batch_size)\n",
    "collate = MyCollator()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print('name: ', name)\n",
    "#     print(type(param))\n",
    "#     print('param.shape: ', param.shape)\n",
    "#     print('param.requires_grad: ', param.requires_grad)\n",
    "#     print('=====')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "#eval_train = [torch.tensor(i, dtype = torch.float32) for i in eval_train]\n",
    "#white_train = [torch.tensor(i, dtype = torch.float32) for i in white_train]\n",
    "\n",
    "#eval_train = torch.stack(eval_train)\n",
    "#white_train = torch.stack(white_train)\n",
    "\n",
    "#train_data = list(zip(evals_packed, white_train))\n",
    "\n",
    "\n",
    "#eval_test = [torch.tensor(i, dtype=torch.float32) for i in eval_test]\n",
    "#white_test = [torch.tensor(i, dtype=torch.float32) for i in white_test]\n",
    "\n",
    "#eval_test = torch.stack(eval_test)\n",
    "#white_test = torch.stack(white_test)\n",
    "\n",
    "\n",
    "# not sure why I'm seeing a warning..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of MyRNN(\n",
      "  (rnn): RNN(2, 256, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=False)\n",
      "  (final): Tanh()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train_data_zip, batch_size=batch_size, shuffle=True ,collate_fn=collate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "learning_rate = .2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "# OK I've figured out the issue, I also need the sequence length for the RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 71 - Learning Rate : 0.2- Avg Loss: 1.022376 - Change in loss: 1.0223764756960527\n",
      "Epoch 2 step 71 - Learning Rate : 0.2- Avg Loss: 1.012528 - Change in loss: 0.9903667756949533\n",
      "Epoch 3 step 71 - Learning Rate : 0.2- Avg Loss: 1.003990 - Change in loss: 0.9915675879657142\n",
      "Epoch 4 step 71 - Learning Rate : 0.2- Avg Loss: 0.968453 - Change in loss: 0.9646049370122722\n",
      "Epoch 5 step 71 - Learning Rate : 0.2- Avg Loss: 0.987186 - Change in loss: 1.0193425530785512\n",
      "Epoch 6 step 71 - Learning Rate : 0.2- Avg Loss: 0.997611 - Change in loss: 1.0105610507411606\n",
      "Epoch 7 step 71 - Learning Rate : 0.2- Avg Loss: 0.971375 - Change in loss: 0.9737006584788809\n",
      "Epoch 8 step 71 - Learning Rate : 0.2- Avg Loss: 0.980111 - Change in loss: 1.0089936646789155\n",
      "Epoch 9 step 71 - Learning Rate : 0.2- Avg Loss: 0.973613 - Change in loss: 0.9933699166723547\n",
      "Epoch 10 step 71 - Learning Rate : 0.1- Avg Loss: 0.964783 - Change in loss: 0.9909304761068054\n",
      "Epoch 11 step 71 - Learning Rate : 0.1- Avg Loss: 0.962811 - Change in loss: 0.9979558592643634\n",
      "Epoch 12 step 71 - Learning Rate : 0.1- Avg Loss: 0.969648 - Change in loss: 1.0071012063461988\n",
      "Epoch 13 step 71 - Learning Rate : 0.1- Avg Loss: 0.968879 - Change in loss: 0.9992067830295294\n",
      "Epoch 14 step 71 - Learning Rate : 0.1- Avg Loss: 0.964753 - Change in loss: 0.9957419373877631\n",
      "Epoch 15 step 71 - Learning Rate : 0.1- Avg Loss: 0.964181 - Change in loss: 0.9994072479577243\n",
      "Epoch 16 step 71 - Learning Rate : 0.1- Avg Loss: 0.965969 - Change in loss: 1.001854451062228\n",
      "Epoch 17 step 71 - Learning Rate : 0.1- Avg Loss: 0.965090 - Change in loss: 0.9990902029500138\n",
      "Epoch 18 step 71 - Learning Rate : 0.1- Avg Loss: 0.969314 - Change in loss: 1.0043760652874683\n",
      "Epoch 19 step 71 - Learning Rate : 0.1- Avg Loss: 0.965314 - Change in loss: 0.9958736229401122\n",
      "Epoch 20 step 71 - Learning Rate : 0.05- Avg Loss: 1.001528 - Change in loss: 1.0375153657870926\n",
      "Epoch 21 step 71 - Learning Rate : 0.05- Avg Loss: 0.966260 - Change in loss: 0.9647858699885169\n",
      "Epoch 22 step 71 - Learning Rate : 0.05- Avg Loss: 0.960006 - Change in loss: 0.9935277785297831\n",
      "Epoch 23 step 71 - Learning Rate : 0.05- Avg Loss: 0.963394 - Change in loss: 1.0035284728783849\n",
      "Epoch 24 step 71 - Learning Rate : 0.05- Avg Loss: 0.961411 - Change in loss: 0.9979426055744847\n",
      "Epoch 25 step 71 - Learning Rate : 0.05- Avg Loss: 0.964827 - Change in loss: 1.0035524433532304\n",
      "Epoch 26 step 71 - Learning Rate : 0.05- Avg Loss: 0.998884 - Change in loss: 1.035298446530306\n",
      "Epoch 27 step 71 - Learning Rate : 0.05- Avg Loss: 0.959760 - Change in loss: 0.9608324021839996\n",
      "Epoch 28 step 71 - Learning Rate : 0.05- Avg Loss: 0.971364 - Change in loss: 1.0120908868630847\n",
      "Epoch 29 step 71 - Learning Rate : 0.05- Avg Loss: 0.960956 - Change in loss: 0.9892847755051205\n",
      "Epoch 30 step 71 - Learning Rate : 0.025- Avg Loss: 0.952977 - Change in loss: 0.9916972227365823\n",
      "Epoch 31 step 71 - Learning Rate : 0.025- Avg Loss: 0.957729 - Change in loss: 1.0049859037561717\n",
      "Epoch 32 step 71 - Learning Rate : 0.025- Avg Loss: 0.979253 - Change in loss: 1.0224739340796425\n",
      "Epoch 33 step 71 - Learning Rate : 0.025- Avg Loss: 0.954701 - Change in loss: 0.974928258643904\n",
      "Epoch 34 step 71 - Learning Rate : 0.025- Avg Loss: 0.952240 - Change in loss: 0.9974222581554819\n",
      "Epoch 35 step 71 - Learning Rate : 0.025- Avg Loss: 0.970866 - Change in loss: 1.0195605945459334\n",
      "Epoch 36 step 71 - Learning Rate : 0.025- Avg Loss: 0.969783 - Change in loss: 0.9988841103717244\n",
      "Epoch 37 step 71 - Learning Rate : 0.025- Avg Loss: 0.953687 - Change in loss: 0.9834026402276329\n",
      "Epoch 38 step 71 - Learning Rate : 0.025- Avg Loss: 0.952541 - Change in loss: 0.9987982613296573\n",
      "Epoch 39 step 71 - Learning Rate : 0.025- Avg Loss: 0.956420 - Change in loss: 1.004072020237209\n",
      "Epoch 40 step 71 - Learning Rate : 0.0125- Avg Loss: 0.949089 - Change in loss: 0.9923347504451444\n",
      "Epoch 41 step 71 - Learning Rate : 0.0125- Avg Loss: 0.943313 - Change in loss: 0.9939141671021955\n",
      "Epoch 42 step 71 - Learning Rate : 0.0125- Avg Loss: 0.944517 - Change in loss: 1.0012763918125944\n",
      "Epoch 43 step 71 - Learning Rate : 0.0125- Avg Loss: 0.941701 - Change in loss: 0.9970185875121546\n",
      "Epoch 44 step 71 - Learning Rate : 0.0125- Avg Loss: 0.941571 - Change in loss: 0.9998622195803788\n",
      "Epoch 45 step 71 - Learning Rate : 0.0125- Avg Loss: 0.947003 - Change in loss: 1.0057689122920441\n",
      "Epoch 46 step 71 - Learning Rate : 0.0125- Avg Loss: 0.939903 - Change in loss: 0.9925029109235269\n",
      "Epoch 47 step 71 - Learning Rate : 0.0125- Avg Loss: 0.936167 - Change in loss: 0.996024717773391\n",
      "Epoch 48 step 71 - Learning Rate : 0.0125- Avg Loss: 0.943132 - Change in loss: 1.0074399418676225\n",
      "Epoch 49 step 71 - Learning Rate : 0.0125- Avg Loss: 0.931746 - Change in loss: 0.9879281402465968\n",
      "Epoch 50 step 71 - Learning Rate : 0.00625- Avg Loss: 0.938077 - Change in loss: 1.0067939863295254\n",
      "Epoch 51 step 71 - Learning Rate : 0.00625- Avg Loss: 0.931096 - Change in loss: 0.9925588647212786\n",
      "Epoch 52 step 71 - Learning Rate : 0.00625- Avg Loss: 0.934654 - Change in loss: 1.0038210343179355\n",
      "Epoch 53 step 71 - Learning Rate : 0.00625- Avg Loss: 0.954351 - Change in loss: 1.0210743291293385\n",
      "Epoch 54 step 71 - Learning Rate : 0.00625- Avg Loss: 0.933950 - Change in loss: 0.9786224523213684\n",
      "Epoch 55 step 71 - Learning Rate : 0.00625- Avg Loss: 0.935111 - Change in loss: 1.0012435095916952\n",
      "Epoch 56 step 71 - Learning Rate : 0.00625- Avg Loss: 0.931653 - Change in loss: 0.9963021044587703\n",
      "Epoch 57 step 71 - Learning Rate : 0.00625- Avg Loss: 0.935406 - Change in loss: 1.0040281949313254\n",
      "Epoch 58 step 71 - Learning Rate : 0.00625- Avg Loss: 0.935745 - Change in loss: 1.0003629482360539\n",
      "Epoch 59 step 71 - Learning Rate : 0.00625- Avg Loss: 0.932193 - Change in loss: 0.9962033913148146\n",
      "Epoch 60 step 71 - Learning Rate : 0.003125- Avg Loss: 0.926073 - Change in loss: 0.9934356751646068\n",
      "Epoch 61 step 71 - Learning Rate : 0.003125- Avg Loss: 0.979871 - Change in loss: 1.0580918311056675\n",
      "Epoch 62 step 71 - Learning Rate : 0.003125- Avg Loss: 0.937051 - Change in loss: 0.9563005586435522\n",
      "Epoch 63 step 71 - Learning Rate : 0.003125- Avg Loss: 0.924017 - Change in loss: 0.9860904932795345\n",
      "Epoch 64 step 71 - Learning Rate : 0.003125- Avg Loss: 0.929309 - Change in loss: 1.005726565404209\n",
      "Epoch 65 step 71 - Learning Rate : 0.003125- Avg Loss: 0.925306 - Change in loss: 0.9956934535887786\n",
      "Epoch 66 step 71 - Learning Rate : 0.003125- Avg Loss: 0.933401 - Change in loss: 1.008748549688395\n",
      "Epoch 67 step 71 - Learning Rate : 0.003125- Avg Loss: 0.924698 - Change in loss: 0.9906757900170122\n",
      "Epoch 68 step 71 - Learning Rate : 0.003125- Avg Loss: 0.933059 - Change in loss: 1.0090413968943286\n",
      "Epoch 69 step 71 - Learning Rate : 0.003125- Avg Loss: 0.923249 - Change in loss: 0.9894858586964072\n",
      "Epoch 70 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.920864 - Change in loss: 0.9974177273750072\n",
      "Epoch 71 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.925260 - Change in loss: 1.004773389818659\n",
      "Epoch 72 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.918912 - Change in loss: 0.9931389393528185\n",
      "Epoch 73 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.946614 - Change in loss: 1.030147065923266\n",
      "Epoch 74 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.927584 - Change in loss: 0.979896150613577\n",
      "Epoch 75 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.921463 - Change in loss: 0.9934014034070145\n",
      "Epoch 76 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.920219 - Change in loss: 0.9986503476663341\n",
      "Epoch 77 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.919327 - Change in loss: 0.9990308034848748\n",
      "Epoch 78 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.921697 - Change in loss: 1.0025774571741324\n",
      "Epoch 79 step 71 - Learning Rate : 0.0015625- Avg Loss: 0.917841 - Change in loss: 0.9958161805145592\n",
      "Epoch 80 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.920297 - Change in loss: 1.002676338216407\n",
      "Epoch 81 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.916591 - Change in loss: 0.9959723491522363\n",
      "Epoch 82 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.917655 - Change in loss: 1.0011612879162135\n",
      "Epoch 83 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.917073 - Change in loss: 0.9993659893179835\n",
      "Epoch 84 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.920205 - Change in loss: 1.003415213552507\n",
      "Epoch 85 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.918550 - Change in loss: 0.9982013177944133\n",
      "Epoch 86 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.918925 - Change in loss: 1.000408543853716\n",
      "Epoch 87 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.919800 - Change in loss: 1.000951449305841\n",
      "Epoch 88 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.919564 - Change in loss: 0.9997433812793152\n",
      "Epoch 89 step 71 - Learning Rate : 0.00078125- Avg Loss: 0.916873 - Change in loss: 0.9970737070956587\n",
      "Epoch 90 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.916402 - Change in loss: 0.9994863154242444\n",
      "Epoch 91 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.914566 - Change in loss: 0.9979970933334514\n",
      "Epoch 92 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.922092 - Change in loss: 1.0082289498601071\n",
      "Epoch 93 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.919143 - Change in loss: 0.9968020420416991\n",
      "Epoch 94 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.915692 - Change in loss: 0.9962448786883874\n",
      "Epoch 95 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.914428 - Change in loss: 0.9986200250538138\n",
      "Epoch 96 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.915418 - Change in loss: 1.001081903495446\n",
      "Epoch 97 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.916229 - Change in loss: 1.000886015945758\n",
      "Epoch 98 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.917378 - Change in loss: 1.0012545419597418\n",
      "Epoch 99 step 71 - Learning Rate : 0.000390625- Avg Loss: 0.915692 - Change in loss: 0.9981617574991323\n",
      "Epoch 100 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.919322 - Change in loss: 1.0039648696296108\n",
      "Epoch 101 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.915852 - Change in loss: 0.9962249151120623\n",
      "Epoch 102 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.915940 - Change in loss: 1.0000967716530909\n",
      "Epoch 103 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.916263 - Change in loss: 1.0003521443079617\n",
      "Epoch 104 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.919088 - Change in loss: 1.0030834656750656\n",
      "Epoch 105 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.915750 - Change in loss: 0.9963675018815795\n",
      "Epoch 106 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.912917 - Change in loss: 0.9969069563563475\n",
      "Epoch 107 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.916181 - Change in loss: 1.0035756331071632\n",
      "Epoch 108 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.915216 - Change in loss: 0.998946801173525\n",
      "Epoch 109 step 71 - Learning Rate : 0.0001953125- Avg Loss: 0.917950 - Change in loss: 1.00298622984001\n",
      "Epoch 110 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.915997 - Change in loss: 0.9978724193657368\n",
      "Epoch 111 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.913370 - Change in loss: 0.9971331504738321\n",
      "Epoch 112 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.914216 - Change in loss: 1.0009258713452631\n",
      "Epoch 113 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.922020 - Change in loss: 1.0085363607458455\n",
      "Epoch 114 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.914189 - Change in loss: 0.9915064876059241\n",
      "Epoch 115 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.915678 - Change in loss: 1.0016282446206668\n",
      "Epoch 116 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.917654 - Change in loss: 1.0021584919631124\n",
      "Epoch 117 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.933941 - Change in loss: 1.017748586244295\n",
      "Epoch 118 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.914261 - Change in loss: 0.9789279608017586\n",
      "Epoch 119 step 71 - Learning Rate : 9.765625e-05- Avg Loss: 0.915128 - Change in loss: 1.0009477439949874\n",
      "Epoch 120 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.919755 - Change in loss: 1.0050566091307682\n",
      "Epoch 121 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.927286 - Change in loss: 1.0081875622351255\n",
      "Epoch 122 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.918140 - Change in loss: 0.9901372977601379\n",
      "Epoch 123 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.915221 - Change in loss: 0.9968210994779448\n",
      "Epoch 124 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 1.031965 - Change in loss: 1.127558299527236\n",
      "Epoch 125 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.913300 - Change in loss: 0.8850103856581398\n",
      "Epoch 126 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.912175 - Change in loss: 0.9987676134409799\n",
      "Epoch 127 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.912671 - Change in loss: 1.000543798097841\n",
      "Epoch 128 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.914123 - Change in loss: 1.0015912699114924\n",
      "Epoch 129 step 71 - Learning Rate : 4.8828125e-05- Avg Loss: 0.914004 - Change in loss: 0.9998695013803319\n",
      "Epoch 130 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.911564 - Change in loss: 0.9973312708242211\n",
      "Epoch 131 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.912829 - Change in loss: 1.0013873904926713\n",
      "Epoch 132 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.913358 - Change in loss: 1.0005797093073598\n",
      "Epoch 133 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.914152 - Change in loss: 1.0008693821117969\n",
      "Epoch 134 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.916221 - Change in loss: 1.002262447781681\n",
      "Epoch 135 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.916082 - Change in loss: 0.9998491338304045\n",
      "Epoch 136 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.913841 - Change in loss: 0.9975538952300061\n",
      "Epoch 137 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.914350 - Change in loss: 1.0005561043504394\n",
      "Epoch 138 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.915844 - Change in loss: 1.0016339225302078\n",
      "Epoch 139 step 71 - Learning Rate : 2.44140625e-05- Avg Loss: 0.914324 - Change in loss: 0.998340417991443\n",
      "Epoch 140 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.915798 - Change in loss: 1.001611903222784\n",
      "Epoch 141 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.912166 - Change in loss: 0.9960345467456315\n",
      "Epoch 142 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.913904 - Change in loss: 1.0019050670300442\n",
      "Epoch 143 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.911618 - Change in loss: 0.9974987396407687\n",
      "Epoch 144 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.912836 - Change in loss: 1.001335820320283\n",
      "Epoch 145 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.915943 - Change in loss: 1.0034042992499597\n",
      "Epoch 146 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.912080 - Change in loss: 0.995781843827233\n",
      "Epoch 147 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.913716 - Change in loss: 1.001794550058254\n",
      "Epoch 148 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.912381 - Change in loss: 0.9985388543390046\n",
      "Epoch 149 step 71 - Learning Rate : 1.220703125e-05- Avg Loss: 0.912078 - Change in loss: 0.9996679101505236\n",
      "Epoch 150 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.912286 - Change in loss: 1.0002275339776032\n",
      "Epoch 151 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.914192 - Change in loss: 1.002088988888461\n",
      "Epoch 152 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.915714 - Change in loss: 1.001664873936909\n",
      "Epoch 153 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 1.006590 - Change in loss: 1.0992410159226973\n",
      "Epoch 154 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.933551 - Change in loss: 0.9274395466296518\n",
      "Epoch 155 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.920977 - Change in loss: 0.9865311896967581\n",
      "Epoch 156 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.912671 - Change in loss: 0.9909812022712671\n",
      "Epoch 157 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.912253 - Change in loss: 0.9995414348917756\n",
      "Epoch 158 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.912777 - Change in loss: 1.0005749810255011\n",
      "Epoch 159 step 71 - Learning Rate : 6.103515625e-06- Avg Loss: 0.911607 - Change in loss: 0.9987180508126985\n",
      "Epoch 160 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.912035 - Change in loss: 1.0004688938863722\n",
      "Epoch 161 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.913920 - Change in loss: 1.0020672591018052\n",
      "Epoch 162 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.911375 - Change in loss: 0.997215610831184\n",
      "Epoch 163 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.921012 - Change in loss: 1.0105735897040748\n",
      "Epoch 164 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.912652 - Change in loss: 0.9909230502283118\n",
      "Epoch 165 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.911801 - Change in loss: 0.9990681213661768\n",
      "Epoch 166 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.911824 - Change in loss: 1.000024867292846\n",
      "Epoch 167 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.916087 - Change in loss: 1.0046746868683964\n",
      "Epoch 168 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.914499 - Change in loss: 0.9982668055218922\n",
      "Epoch 169 step 71 - Learning Rate : 3.0517578125e-06- Avg Loss: 0.912279 - Change in loss: 0.9975721064281746\n",
      "Epoch 170 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.913973 - Change in loss: 1.0018568829844667\n",
      "Epoch 171 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.913841 - Change in loss: 0.999856632798702\n",
      "Epoch 172 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.914283 - Change in loss: 1.0004831612574223\n",
      "Epoch 173 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.912538 - Change in loss: 0.9980909573392764\n",
      "Epoch 174 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.912915 - Change in loss: 1.0004133968160842\n",
      "Epoch 175 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.913229 - Change in loss: 1.0003439069744493\n",
      "Epoch 176 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.913902 - Change in loss: 1.0007367543245962\n",
      "Epoch 177 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.914130 - Change in loss: 1.000249959820897\n",
      "Epoch 178 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.917513 - Change in loss: 1.0037005561943835\n",
      "Epoch 179 step 71 - Learning Rate : 1.52587890625e-06- Avg Loss: 0.912324 - Change in loss: 0.9943446561820952\n",
      "Epoch 180 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.911929 - Change in loss: 0.9995665950716298\n",
      "Epoch 181 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.914004 - Change in loss: 1.0022759075007974\n",
      "Epoch 182 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.911509 - Change in loss: 0.9972706851162774\n",
      "Epoch 183 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.915325 - Change in loss: 1.0041862068970373\n",
      "Epoch 184 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.919580 - Change in loss: 1.004648109148573\n",
      "Epoch 185 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.914858 - Change in loss: 0.9948652223191939\n",
      "Epoch 186 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.917352 - Change in loss: 1.0027259877126231\n",
      "Epoch 187 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.912555 - Change in loss: 0.9947708121719171\n",
      "Epoch 188 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.913258 - Change in loss: 1.0007706119147735\n",
      "Epoch 189 step 71 - Learning Rate : 7.62939453125e-07- Avg Loss: 0.914643 - Change in loss: 1.0015165730204176\n",
      "Epoch 190 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.911566 - Change in loss: 0.9966354831794658\n",
      "Epoch 191 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.916261 - Change in loss: 1.005150329871828\n",
      "Epoch 192 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.914649 - Change in loss: 0.9982416547125682\n",
      "Epoch 193 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.915127 - Change in loss: 1.0005221133569993\n",
      "Epoch 194 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.914597 - Change in loss: 0.999421175016561\n",
      "Epoch 195 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.915124 - Change in loss: 1.0005759012319388\n",
      "Epoch 196 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.914557 - Change in loss: 0.9993802391073202\n",
      "Epoch 197 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.912017 - Change in loss: 0.9972230492953307\n",
      "Epoch 198 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.912736 - Change in loss: 1.000788616817276\n",
      "Epoch 199 step 71 - Learning Rate : 3.814697265625e-07- Avg Loss: 0.914108 - Change in loss: 1.0015022276265408\n",
      "Epoch 200 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.917327 - Change in loss: 1.0035216080373879\n",
      "Epoch 201 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.916114 - Change in loss: 0.9986774586961367\n",
      "Epoch 202 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.929144 - Change in loss: 1.0142231252718459\n",
      "Epoch 203 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.912669 - Change in loss: 0.9822687734808832\n",
      "Epoch 204 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.922886 - Change in loss: 1.011194880383286\n",
      "Epoch 205 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.913795 - Change in loss: 0.9901500077192994\n",
      "Epoch 206 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.916653 - Change in loss: 1.0031273133193301\n",
      "Epoch 207 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.918913 - Change in loss: 1.002465104859199\n",
      "Epoch 208 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.912881 - Change in loss: 0.9934364316409862\n",
      "Epoch 209 step 71 - Learning Rate : 1.9073486328125e-07- Avg Loss: 0.948041 - Change in loss: 1.0385153717317637\n",
      "Epoch 210 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.913293 - Change in loss: 0.9633469875058192\n",
      "Epoch 211 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.913451 - Change in loss: 1.0001733342495127\n",
      "Epoch 212 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.913506 - Change in loss: 1.0000599040289082\n",
      "Epoch 213 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.914549 - Change in loss: 1.0011422621210564\n",
      "Epoch 214 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.916375 - Change in loss: 1.0019964491756772\n",
      "Epoch 215 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.913600 - Change in loss: 0.9969719515654767\n",
      "Epoch 216 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.916142 - Change in loss: 1.0027823360467278\n",
      "Epoch 217 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.914827 - Change in loss: 0.9985642512523819\n",
      "Epoch 218 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.915607 - Change in loss: 1.0008528691191478\n",
      "Epoch 219 step 71 - Learning Rate : 9.5367431640625e-08- Avg Loss: 0.923581 - Change in loss: 1.0087083352426809\n",
      "Epoch 220 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.924353 - Change in loss: 1.0008360971065928\n",
      "Epoch 221 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.928270 - Change in loss: 1.0042373425129987\n",
      "Epoch 222 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.912057 - Change in loss: 0.982535026901854\n",
      "Epoch 223 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.914754 - Change in loss: 1.0029563730273694\n",
      "Epoch 224 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.912931 - Change in loss: 0.998007526197682\n",
      "Epoch 225 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.911772 - Change in loss: 0.998730099635192\n",
      "Epoch 226 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.911775 - Change in loss: 1.0000033636395773\n",
      "Epoch 227 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.915702 - Change in loss: 1.004307572389175\n",
      "Epoch 228 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.940847 - Change in loss: 1.0274593026371763\n",
      "Epoch 229 step 71 - Learning Rate : 4.76837158203125e-08- Avg Loss: 0.937647 - Change in loss: 0.9965985580984944\n",
      "Epoch 230 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.914392 - Change in loss: 0.975198739661828\n",
      "Epoch 231 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.913520 - Change in loss: 0.9990459166185512\n",
      "Epoch 232 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.915478 - Change in loss: 1.0021439900712836\n",
      "Epoch 233 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.932878 - Change in loss: 1.0190067254162152\n",
      "Epoch 234 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.911269 - Change in loss: 0.9768358389072794\n",
      "Epoch 235 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.921509 - Change in loss: 1.0112375638750504\n",
      "Epoch 236 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.913479 - Change in loss: 0.9912854519967226\n",
      "Epoch 237 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.911525 - Change in loss: 0.9978610184907555\n",
      "Epoch 238 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.922629 - Change in loss: 1.0121818913449476\n",
      "Epoch 239 step 71 - Learning Rate : 2.384185791015625e-08- Avg Loss: 0.917161 - Change in loss: 0.9940729076311395\n",
      "Epoch 240 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.911140 - Change in loss: 0.9934359743293053\n",
      "Epoch 241 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.915992 - Change in loss: 1.0053248560242254\n",
      "Epoch 242 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.911375 - Change in loss: 0.9949595157320019\n",
      "Epoch 243 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.912832 - Change in loss: 1.0015987081738433\n",
      "Epoch 244 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.913800 - Change in loss: 1.0010600482622034\n",
      "Epoch 245 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.916018 - Change in loss: 1.002427548541381\n",
      "Epoch 246 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.925864 - Change in loss: 1.010749319850019\n",
      "Epoch 247 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.911975 - Change in loss: 0.984998321234055\n",
      "Epoch 248 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.911688 - Change in loss: 0.9996848588699849\n",
      "Epoch 249 step 71 - Learning Rate : 1.1920928955078126e-08- Avg Loss: 0.912749 - Change in loss: 1.00116379738945\n",
      "Epoch 250 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.913769 - Change in loss: 1.001117845062806\n",
      "Epoch 251 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.912822 - Change in loss: 0.9989632670316483\n",
      "Epoch 252 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.913656 - Change in loss: 1.000914420202769\n",
      "Epoch 253 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.913376 - Change in loss: 0.999693370036812\n",
      "Epoch 254 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.916586 - Change in loss: 1.0035137811501225\n",
      "Epoch 255 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.913093 - Change in loss: 0.9961894390665404\n",
      "Epoch 256 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.914272 - Change in loss: 1.001291721595183\n",
      "Epoch 257 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.912804 - Change in loss: 0.9983937444956172\n",
      "Epoch 258 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.917106 - Change in loss: 1.0047127182087259\n",
      "Epoch 259 step 71 - Learning Rate : 5.960464477539063e-09- Avg Loss: 0.912262 - Change in loss: 0.9947183567485852\n",
      "Epoch 260 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.919031 - Change in loss: 1.007420666390184\n",
      "Epoch 261 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.913899 - Change in loss: 0.9944155380660826\n",
      "Epoch 262 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.936070 - Change in loss: 1.0242597060212457\n",
      "Epoch 263 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.911924 - Change in loss: 0.9742048534819114\n",
      "Epoch 264 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.916235 - Change in loss: 1.00472749137029\n",
      "Epoch 265 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.918011 - Change in loss: 1.0019385124173277\n",
      "Epoch 266 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.919037 - Change in loss: 1.001117147615982\n",
      "Epoch 267 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.913840 - Change in loss: 0.9943455242129451\n",
      "Epoch 268 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.914933 - Change in loss: 1.0011960967187332\n",
      "Epoch 269 step 71 - Learning Rate : 2.9802322387695314e-09- Avg Loss: 0.931024 - Change in loss: 1.0175873487649074\n",
      "Epoch 270 step 71 - Learning Rate : 1.4901161193847657e-09- Avg Loss: 0.912969 - Change in loss: 0.9806071215642899\n",
      "Epoch 271 step 71 - Learning Rate : 1.4901161193847657e-09- Avg Loss: 0.913213 - Change in loss: 1.0002669931720816\n",
      "Epoch 272 step 71 - Learning Rate : 1.4901161193847657e-09- Avg Loss: 0.938940 - Change in loss: 1.0281721080535413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [277]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# optimizeing\u001B[39;00m\n\u001B[0;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 25\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     27\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "avg_losses = []\n",
    "epochs = []\n",
    "avg_loss = 1\n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    if (epoch+1) % 10 ==0:\n",
    "        learning_rate /= 2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    i = 0\n",
    "    losses = []\n",
    "    for evals, elo in data_loader:\n",
    "        evals = evals.to(device)\n",
    "\n",
    "        elo = elo.to(device)\n",
    "        i +=1\n",
    "        outputs = model(evals)\n",
    "        #print(outputs)\n",
    "        #print(outputs.shape, elo.shape)\n",
    "        loss = criterion(outputs,elo)\n",
    "\n",
    "        # optimizeing\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    change = stats.mean(losses)/avg_loss\n",
    "    avg_loss = stats.mean(losses)\n",
    "    avg_losses.append(avg_loss)\n",
    "    epochs.append(epoch)\n",
    "    print(f'Epoch {epoch+1} step {i+1} - Learning Rate : {learning_rate}- Avg Loss: {avg_loss:3f} - Change in loss: {change}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Why are 4/5 always zero???"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x17dd359b070>"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(epochs, avg_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "data": {
      "text/plain": "MyRNN(\n  (rnn): RNN(2, 256, num_layers=3, batch_first=True)\n  (fc): Linear(in_features=256, out_features=1, bias=False)\n  (final): Tanh()\n)"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "test_data_loader = torch.utils.data.DataLoader(test_data_zip, batch_size=1, shuffle=False ,collate_fn=collate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 1.1889135271793403\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "outputs = []\n",
    "elos = []\n",
    "for evals, elo in test_data_loader:\n",
    "    #print(\"evals\",evals.shape)\n",
    "    evals = evals.to(device)\n",
    "    elo = elo.to(device)\n",
    "    output = model(evals)\n",
    "    outputs.append(output.item())\n",
    "    elos.append(elo.item())\n",
    "    loss = criterion(output,elo)\n",
    "    #print(f'Model prediction : {output} \\n ELO : {elo} \\n MSE : {loss}')\n",
    "    losses.append(loss.item())\n",
    "print(f'Average loss : {stats.mean(losses)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x17dd36c99c0>"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(elos,outputs, alpha = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6828564405441284\n",
      "-0.597422182559967\n"
     ]
    }
   ],
   "source": [
    "print(max(outputs))\n",
    "print(min(outputs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7390,
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, no_layers):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.no_layers = no_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, no_layers, batch_first = True, bias = False)\n",
    "        self.fc = nn.Linear(hidden_size,1, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial hidden state\n",
    "        h0 = torch.rand(self.no_layers,x.size(0),self.hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x,h0)\n",
    "        #print(out) #return out\n",
    "        out = out[:,-1,:]\n",
    "        #print(out)\n",
    "        out = self.fc(out)\n",
    "        #print(out)\n",
    "        m = nn.Sigmoid()\n",
    "        out = m(out)\n",
    "        return out\n",
    "        # print(\"out: \",out)\n",
    "        # print('RNN WEIGHTS: ',self.rnn.weight_ih_l0  )\n",
    "        # print(self.fc.weight)\n",
    "        #\n",
    "        # #print(\"Count these...\",out)\n",
    "        # m = nn.Sigmoid()\n",
    "        # out = m(out)\n",
    "        # if x.size(0) == 1:\n",
    "        #     return out[0]\n",
    "        # else:\n",
    "        #     return torch.squeeze(out)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7338,
   "outputs": [],
   "source": [
    "model = MyRNN(input_size, hidden_size, no_layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7339,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 132, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 2, got 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7339]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m sample_input \u001B[38;5;241m=\u001B[39m sample_input\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(sample_input\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_input\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [7337]\u001B[0m, in \u001B[0;36mMyRNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# initial hidden state\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mno_layers,x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m),\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\n\u001B[1;32m---> 13\u001B[0m     out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43mh0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m#print(out) #return out\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     out \u001B[38;5;241m=\u001B[39m out[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:472\u001B[0m, in \u001B[0;36mRNN.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    469\u001B[0m     hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m hx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 472\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_forward_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRNN_TANH\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRNN_RELU\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:234\u001B[0m, in \u001B[0;36mRNNBase.check_forward_args\u001B[1;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m     expected_hidden_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_hidden_size(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:210\u001B[0m, in \u001B[0;36mRNNBase.check_input\u001B[1;34m(self, input, batch_sizes)\u001B[0m\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    207\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput must have \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m dimensions, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    208\u001B[0m             expected_input_dim, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    211\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput.size(-1) must be equal to input_size. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    212\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: input.size(-1) must be equal to input_size. Expected 2, got 1"
     ]
    }
   ],
   "source": [
    "d = [i*2/150**2 for i in range(132)]\n",
    "sample_input = torch.tensor(d, dtype = torch.float32)\n",
    "sample_input = sample_input.unsqueeze(1)\n",
    "sample_input = sample_input.unsqueeze(0)\n",
    "print(sample_input.shape)\n",
    "print(model(sample_input))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = [i+1/132 for i in range(132)]\n",
    "sample_input = torch.tensor(d, dtype = torch.float32)\n",
    "sample_input = sample_input.unsqueeze(1)\n",
    "sample_input = sample_input.unsqueeze(0)\n",
    "print(sample_input.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model(sample_input))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
